{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN with tensorflow saver and restore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jk/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "\"\"\"A very simple MNIST classifier.\n",
    "\"\"\"\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "slim = tf.contrib.slim\n",
    "\n",
    "sess_config = tf.ConfigProto(gpu_options=tf.GPUOptions(allow_growth=True))\n",
    "\n",
    "np.random.seed(1)\n",
    "tf.set_random_seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load training and eval data from tf.keras\n",
    "(train_data, train_labels), (test_data, test_labels) = \\\n",
    "    tf.keras.datasets.mnist.load_data()\n",
    "\n",
    "train_data = train_data / 255.\n",
    "train_labels = np.asarray(train_labels, dtype=np.int32)\n",
    "\n",
    "test_data = test_data / 255.\n",
    "test_labels = np.asarray(test_labels, dtype=np.int32)\n",
    "\n",
    "# # for fast learning with small dataset\n",
    "# N = 200\n",
    "# train_data = train_data[:N]\n",
    "# train_labels = train_labels[:N]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label = 5\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "index = 0\n",
    "print(\"label = {}\".format(train_labels[index]))\n",
    "plt.imshow(train_data[index].reshape(28, 28), cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<BatchDataset shapes: ((?, 28, 28), (?,)), types: (tf.float64, tf.int32)>\n",
      "<BatchDataset shapes: ((?, 28, 28), (?,)), types: (tf.float64, tf.int32)>\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "\n",
    "# for train\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((train_data, train_labels))\n",
    "train_dataset = train_dataset.shuffle(buffer_size = 10000)\n",
    "train_dataset = train_dataset.batch(batch_size = batch_size)\n",
    "print(train_dataset)\n",
    "\n",
    "# for test\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((test_data, test_labels))\n",
    "test_dataset = test_dataset.shuffle(buffer_size = 10000)\n",
    "test_dataset = test_dataset.batch(batch_size = len(test_data))\n",
    "print(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf.data.Iterator.from_string_handle의 output_shapes는 default = None이지만 꼭 값을 넣는 게 좋음\n",
    "handle = tf.placeholder(tf.string, shape=[])\n",
    "iterator = tf.data.Iterator.from_string_handle(handle,\n",
    "                                               train_dataset.output_types,\n",
    "                                               train_dataset.output_shapes)\n",
    "x, y = iterator.get_next()\n",
    "x = tf.cast(x, dtype = tf.float32)\n",
    "y = tf.cast(y, dtype = tf.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cnn_model_fn(x):\n",
    "    \"\"\"Model function for CNN.\n",
    "    Args:\n",
    "    x: input images\n",
    "    mode: boolean whether trainig mode or test mode\n",
    "\n",
    "    Returns:\n",
    "    logits: unnormalized score funtion\n",
    "    \"\"\"\n",
    "    # Input Layer\n",
    "    # Reshape X to 4-D tensor: [batch_size, width, height, channels]\n",
    "    # MNIST images are 28x28 pixels, and have one color channel\n",
    "    with tf.name_scope('reshape'):\n",
    "        x_image = tf.reshape(x, [-1, 28, 28, 1])\n",
    "\n",
    "    # Convolutional Layer #1\n",
    "    # Input Tensor Shape: [batch_size, 28, 28, 1]\n",
    "    # Output Tensor Shape: [batch_size, 28, 28, 32]\n",
    "    conv1 = slim.conv2d(x_image, 32, [5, 5], scope='conv1')\n",
    "\n",
    "\n",
    "    # Pooling Layer #1\n",
    "    # Input Tensor Shape: [batch_size, 28, 28, 32]\n",
    "    # Output Tensor Shape: [batch_size, 14, 14, 32]\n",
    "    pool1 = slim.max_pool2d(conv1, [2, 2], scope='pool1')\n",
    "\n",
    "    # Convolutional Layer #2\n",
    "    # Input Tensor Shape: [batch_size, 14, 14, 32]\n",
    "    # Output Tensor Shape: [batch_size, 14, 14, 64]\n",
    "    conv2 = slim.conv2d(pool1, 64, [5, 5], scope='conv2')\n",
    "\n",
    "    # Pooling Layer #2\n",
    "    # Second max pooling layer with a 2x2 filter and stride of 2\n",
    "    # Input Tensor Shape: [batch_size, 14, 14, 64]\n",
    "    # Output Tensor Shape: [batch_size, 7, 7, 64]\n",
    "    pool2 = slim.max_pool2d(conv2, [2, 2], scope='pool2')\n",
    "\n",
    "    # Flatten tensor into a batch of vectors\n",
    "    # Input Tensor Shape: [batch_size, 7, 7, 64]\n",
    "    # Output Tensor Shape: [batch_size, 7 * 7 * 64]\n",
    "    pool2_flat = slim.flatten(pool2, scope='flatten')\n",
    "\n",
    "    # Fully connected Layer\n",
    "    # Input Tensor Shape: [batch_size, 7 * 7 * 64]\n",
    "    # Output Tensor Shape: [batch_size, 1024]\n",
    "    fc1 = slim.fully_connected(pool2_flat, 1024, scope='fc1')\n",
    "\n",
    "    # Add dropout operation; 0.6 probability that element will be kept\n",
    "    is_training = tf.placeholder(tf.bool)\n",
    "    fc1_drop = slim.dropout(fc1, keep_prob=0.6, is_training=is_training, scope='dropout')\n",
    "\n",
    "    # Logits layer\n",
    "    # Input Tensor Shape: [batch_size, 1024]\n",
    "    # Output Tensor Shape: [batch_size, 10]\n",
    "    logits = slim.fully_connected(fc1_drop, 10, activation_fn=None, scope='logits')\n",
    "\n",
    "    return logits, is_training, x_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits, is_training, x_image = cnn_model_fn(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_entropy = tf.losses.sparse_softmax_cross_entropy(labels=y, logits=logits)\n",
    "\n",
    "train_op = tf.train.AdamOptimizer(1e-4).minimize(cross_entropy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving graph to: graphs/train/\n"
     ]
    }
   ],
   "source": [
    "train_dir = 'graphs/train/'\n",
    "print('Saving graph to: %s' % train_dir)\n",
    "train_writer = tf.summary.FileWriter(train_dir)\n",
    "train_writer.add_graph(tf.get_default_graph()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope('summaries'):\n",
    "    tf.summary.scalar('loss/cross_entropy', cross_entropy)\n",
    "    tf.summary.image('images', x_image)\n",
    "    for var in tf.trainable_variables():\n",
    "        tf.summary.histogram(var.op.name, var)\n",
    "    # merge all summaries\n",
    "    summary_op = tf.summary.merge_all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# tf saver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a saver object\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 0, loss: 2.3054018020629883\n",
      "step: 10, loss: 2.152693271636963\n",
      "step: 20, loss: 1.9872583150863647\n",
      "step: 30, loss: 1.8255133628845215\n",
      "step: 40, loss: 1.6476857662200928\n",
      "step: 50, loss: 1.385237216949463\n",
      "step: 60, loss: 1.0713540315628052\n",
      "step: 70, loss: 0.9330483675003052\n",
      "step: 80, loss: 0.6298819780349731\n",
      "step: 90, loss: 0.4742966294288635\n",
      "step: 100, loss: 0.5777295827865601\n",
      "step: 110, loss: 0.43588483333587646\n",
      "step: 120, loss: 0.5337479114532471\n",
      "step: 130, loss: 0.7679741382598877\n",
      "step: 140, loss: 0.47654175758361816\n",
      "step: 150, loss: 0.4634859561920166\n",
      "step: 160, loss: 0.5241920948028564\n",
      "step: 170, loss: 0.6299886703491211\n",
      "step: 180, loss: 0.35555118322372437\n",
      "step: 190, loss: 0.4733356833457947\n",
      "step: 200, loss: 0.2536178529262543\n",
      "step: 210, loss: 0.3214131295681\n",
      "step: 220, loss: 0.3674960732460022\n",
      "step: 230, loss: 0.4189661741256714\n",
      "step: 240, loss: 0.6117838025093079\n",
      "step: 250, loss: 0.13731563091278076\n",
      "step: 260, loss: 0.4073445200920105\n",
      "step: 270, loss: 0.5219544172286987\n",
      "step: 280, loss: 0.3376997411251068\n",
      "step: 290, loss: 0.26605451107025146\n",
      "step: 300, loss: 0.3273886740207672\n",
      "step: 310, loss: 0.5050318241119385\n",
      "step: 320, loss: 0.16582711040973663\n",
      "step: 330, loss: 0.5166558623313904\n",
      "step: 340, loss: 0.17391923069953918\n",
      "step: 350, loss: 0.12240248173475266\n",
      "step: 360, loss: 0.4655366539955139\n",
      "step: 370, loss: 0.2476331889629364\n",
      "step: 380, loss: 0.15810784697532654\n",
      "step: 390, loss: 0.23911535739898682\n",
      "step: 400, loss: 0.4027552902698517\n",
      "step: 410, loss: 0.16739314794540405\n",
      "step: 420, loss: 0.14715476334095\n",
      "step: 430, loss: 0.2548072636127472\n",
      "step: 440, loss: 0.2253975123167038\n",
      "step: 450, loss: 0.12321918457746506\n",
      "step: 460, loss: 0.33462604880332947\n",
      "step: 470, loss: 0.4612119197845459\n",
      "step: 480, loss: 0.18665900826454163\n",
      "step: 490, loss: 0.13790197670459747\n",
      "step: 500, loss: 0.5035176277160645\n",
      "step: 510, loss: 0.17304831743240356\n",
      "step: 520, loss: 0.2658390402793884\n",
      "step: 530, loss: 0.19678828120231628\n",
      "step: 540, loss: 0.19532276690006256\n",
      "step: 550, loss: 0.5264743566513062\n",
      "step: 560, loss: 0.41764384508132935\n",
      "step: 570, loss: 0.18466979265213013\n",
      "step: 580, loss: 0.19489425420761108\n",
      "step: 590, loss: 0.23916055262088776\n",
      "step: 600, loss: 0.2945607602596283\n",
      "step: 610, loss: 0.25374338030815125\n",
      "step: 620, loss: 0.274863064289093\n",
      "step: 630, loss: 0.07408088445663452\n",
      "step: 640, loss: 0.17874184250831604\n",
      "step: 650, loss: 0.14899849891662598\n",
      "step: 660, loss: 0.18070197105407715\n",
      "step: 670, loss: 0.06736835837364197\n",
      "step: 680, loss: 0.10554087907075882\n",
      "step: 690, loss: 0.06846469640731812\n",
      "step: 700, loss: 0.13063114881515503\n",
      "step: 710, loss: 0.19931992888450623\n",
      "step: 720, loss: 0.034670390188694\n",
      "step: 730, loss: 0.06890316307544708\n",
      "step: 740, loss: 0.13048233091831207\n",
      "step: 750, loss: 0.11373133212327957\n",
      "step: 760, loss: 0.04125075414776802\n",
      "step: 770, loss: 0.1254875361919403\n",
      "step: 780, loss: 0.25085175037384033\n",
      "step: 790, loss: 0.2314925193786621\n",
      "step: 800, loss: 0.10621565580368042\n",
      "step: 810, loss: 0.134302020072937\n",
      "step: 820, loss: 0.1126318871974945\n",
      "step: 830, loss: 0.1377856731414795\n",
      "step: 840, loss: 0.0451383963227272\n",
      "step: 850, loss: 0.1168600395321846\n",
      "step: 860, loss: 0.2598488926887512\n",
      "step: 870, loss: 0.23293016850948334\n",
      "step: 880, loss: 0.09843216091394424\n",
      "step: 890, loss: 0.15181206166744232\n",
      "step: 900, loss: 0.12930327653884888\n",
      "step: 910, loss: 0.2944756746292114\n",
      "step: 920, loss: 0.14473594725131989\n",
      "step: 930, loss: 0.35131213068962097\n",
      "step: 940, loss: 0.10359306633472443\n",
      "step: 950, loss: 0.25530606508255005\n",
      "step: 960, loss: 0.11160644888877869\n",
      "step: 970, loss: 0.07438773661851883\n",
      "step: 980, loss: 0.15489594638347626\n",
      "step: 990, loss: 0.044031038880348206\n",
      "step: 1000, loss: 0.08990900218486786\n",
      "step: 1010, loss: 0.0586681105196476\n",
      "step: 1020, loss: 0.08115741610527039\n",
      "step: 1030, loss: 0.22932523488998413\n",
      "step: 1040, loss: 0.027749286964535713\n",
      "step: 1050, loss: 0.13787482678890228\n",
      "step: 1060, loss: 0.0808541476726532\n",
      "step: 1070, loss: 0.20679119229316711\n",
      "step: 1080, loss: 0.08836175501346588\n",
      "step: 1090, loss: 0.2265630066394806\n",
      "step: 1100, loss: 0.12070468068122864\n",
      "step: 1110, loss: 0.4185607135295868\n",
      "step: 1120, loss: 0.08387155085802078\n",
      "step: 1130, loss: 0.02924468368291855\n",
      "step: 1140, loss: 0.13319043815135956\n",
      "step: 1150, loss: 0.042293839156627655\n",
      "step: 1160, loss: 0.09993123263120651\n",
      "step: 1170, loss: 0.06186569482088089\n",
      "step: 1180, loss: 0.039503976702690125\n",
      "step: 1190, loss: 0.1379953771829605\n",
      "step: 1200, loss: 0.03114660084247589\n",
      "step: 1210, loss: 0.16319262981414795\n",
      "step: 1220, loss: 0.06886100769042969\n",
      "step: 1230, loss: 0.12379869818687439\n",
      "step: 1240, loss: 0.11146266013383865\n",
      "step: 1250, loss: 0.12950339913368225\n",
      "step: 1260, loss: 0.17317219078540802\n",
      "step: 1270, loss: 0.21371713280677795\n",
      "step: 1280, loss: 0.010210087522864342\n",
      "step: 1290, loss: 0.1110992506146431\n",
      "step: 1300, loss: 0.1284404695034027\n",
      "step: 1310, loss: 0.043864209204912186\n",
      "step: 1320, loss: 0.04759592562913895\n",
      "step: 1330, loss: 0.010097159072756767\n",
      "step: 1340, loss: 0.07043129205703735\n",
      "step: 1350, loss: 0.14426976442337036\n",
      "step: 1360, loss: 0.06774640083312988\n",
      "step: 1370, loss: 0.013988161459565163\n",
      "step: 1380, loss: 0.06675492227077484\n",
      "step: 1390, loss: 0.03362701088190079\n",
      "step: 1400, loss: 0.13315463066101074\n",
      "step: 1410, loss: 0.022960763424634933\n",
      "step: 1420, loss: 0.12855267524719238\n",
      "step: 1430, loss: 0.02618468552827835\n",
      "step: 1440, loss: 0.09701044857501984\n",
      "step: 1450, loss: 0.03564973548054695\n",
      "step: 1460, loss: 0.14615270495414734\n",
      "step: 1470, loss: 0.10080496966838837\n",
      "step: 1480, loss: 0.3233162760734558\n",
      "step: 1490, loss: 0.05144508183002472\n",
      "step: 1500, loss: 0.09003616124391556\n",
      "step: 1510, loss: 0.11414676904678345\n",
      "step: 1520, loss: 0.11385729908943176\n",
      "step: 1530, loss: 0.04038006812334061\n",
      "step: 1540, loss: 0.211941659450531\n",
      "step: 1550, loss: 0.033219750970602036\n",
      "step: 1560, loss: 0.08119161427021027\n",
      "step: 1570, loss: 0.28823041915893555\n",
      "step: 1580, loss: 0.024133505299687386\n",
      "step: 1590, loss: 0.08590343594551086\n",
      "step: 1600, loss: 0.0507054403424263\n",
      "step: 1610, loss: 0.20870965719223022\n",
      "step: 1620, loss: 0.1979537159204483\n",
      "step: 1630, loss: 0.08119349181652069\n",
      "step: 1640, loss: 0.05865347757935524\n",
      "step: 1650, loss: 0.02181851491332054\n",
      "step: 1660, loss: 0.07026273012161255\n",
      "step: 1670, loss: 0.04036372900009155\n",
      "step: 1680, loss: 0.04170744866132736\n",
      "step: 1690, loss: 0.12206052243709564\n",
      "step: 1700, loss: 0.0276343934237957\n",
      "End of dataset\n",
      "Save model at 0 epochs\n",
      "Epochs: 0 Elapsed time: 205.66478371620178\n",
      "step: 1710, loss: 0.08048507571220398\n",
      "step: 1720, loss: 0.0363086573779583\n",
      "step: 1730, loss: 0.12344886362552643\n",
      "step: 1740, loss: 0.039933450520038605\n",
      "step: 1750, loss: 0.007419761270284653\n",
      "step: 1760, loss: 0.05142582580447197\n",
      "step: 1770, loss: 0.07620717585086823\n",
      "step: 1780, loss: 0.10861337929964066\n",
      "step: 1790, loss: 0.04468967020511627\n",
      "step: 1800, loss: 0.06540399044752121\n",
      "step: 1810, loss: 0.06631045043468475\n",
      "step: 1820, loss: 0.04116395115852356\n",
      "step: 1830, loss: 0.06977105140686035\n",
      "step: 1840, loss: 0.2137027531862259\n",
      "step: 1850, loss: 0.28729209303855896\n",
      "step: 1860, loss: 0.16381190717220306\n",
      "step: 1870, loss: 0.020201317965984344\n",
      "step: 1880, loss: 0.07168952375650406\n",
      "step: 1890, loss: 0.2653042674064636\n",
      "step: 1900, loss: 0.05716720223426819\n",
      "step: 1910, loss: 0.054144687950611115\n",
      "step: 1920, loss: 0.07270950078964233\n",
      "step: 1930, loss: 0.07989874482154846\n",
      "step: 1940, loss: 0.06814603507518768\n",
      "step: 1950, loss: 0.1255444586277008\n",
      "step: 1960, loss: 0.15787570178508759\n",
      "step: 1970, loss: 0.09306969493627548\n",
      "step: 1980, loss: 0.02246944047510624\n",
      "step: 1990, loss: 0.023932311683893204\n",
      "step: 2000, loss: 0.2573671042919159\n",
      "step: 2010, loss: 0.04059489816427231\n",
      "step: 2020, loss: 0.08305324614048004\n",
      "step: 2030, loss: 0.07470957189798355\n",
      "step: 2040, loss: 0.18769994378089905\n",
      "step: 2050, loss: 0.2782583236694336\n",
      "step: 2060, loss: 0.02541602961719036\n",
      "step: 2070, loss: 0.0100034698843956\n",
      "step: 2080, loss: 0.01203312911093235\n",
      "step: 2090, loss: 0.0308803953230381\n",
      "step: 2100, loss: 0.020618071779608727\n",
      "step: 2110, loss: 0.11138360947370529\n",
      "step: 2120, loss: 0.07340136170387268\n",
      "step: 2130, loss: 0.043617792427539825\n",
      "step: 2140, loss: 0.3138265609741211\n",
      "step: 2150, loss: 0.07520562410354614\n",
      "step: 2160, loss: 0.1566578596830368\n",
      "step: 2170, loss: 0.11068791151046753\n",
      "step: 2180, loss: 0.11094853281974792\n",
      "step: 2190, loss: 0.043416887521743774\n",
      "step: 2200, loss: 0.016196027398109436\n",
      "step: 2210, loss: 0.10130400955677032\n",
      "step: 2220, loss: 0.029819943010807037\n",
      "step: 2230, loss: 0.049062907695770264\n",
      "step: 2240, loss: 0.04184592142701149\n",
      "step: 2250, loss: 0.01875249668955803\n",
      "step: 2260, loss: 0.09717503190040588\n",
      "step: 2270, loss: 0.012572644278407097\n",
      "step: 2280, loss: 0.00868178904056549\n",
      "step: 2290, loss: 0.22782400250434875\n",
      "step: 2300, loss: 0.061756256967782974\n",
      "step: 2310, loss: 0.03431008383631706\n",
      "step: 2320, loss: 0.011633846908807755\n",
      "step: 2330, loss: 0.020150277763605118\n",
      "step: 2340, loss: 0.036975860595703125\n",
      "step: 2350, loss: 0.015689723193645477\n",
      "step: 2360, loss: 0.141922265291214\n",
      "step: 2370, loss: 0.031897757202386856\n",
      "step: 2380, loss: 0.23138533532619476\n",
      "step: 2390, loss: 0.017358627170324326\n",
      "step: 2400, loss: 0.006179953459650278\n",
      "step: 2410, loss: 0.16768725216388702\n",
      "step: 2420, loss: 0.12779811024665833\n",
      "step: 2430, loss: 0.1327492594718933\n",
      "step: 2440, loss: 0.23558758199214935\n",
      "step: 2450, loss: 0.0814121663570404\n",
      "step: 2460, loss: 0.14335785806179047\n",
      "step: 2470, loss: 0.05439215153455734\n",
      "step: 2480, loss: 0.04664254188537598\n",
      "step: 2490, loss: 0.021217048168182373\n",
      "step: 2500, loss: 0.08499231189489365\n",
      "step: 2510, loss: 0.005154754966497421\n",
      "step: 2520, loss: 0.027328649535775185\n",
      "step: 2530, loss: 0.015771763399243355\n",
      "step: 2540, loss: 0.1241503581404686\n",
      "step: 2550, loss: 0.0592905730009079\n",
      "step: 2560, loss: 0.01609482802450657\n",
      "step: 2570, loss: 0.07127977162599564\n",
      "step: 2580, loss: 0.06557945907115936\n",
      "step: 2590, loss: 0.04122760891914368\n",
      "step: 2600, loss: 0.16005393862724304\n",
      "step: 2610, loss: 0.01161249540746212\n",
      "step: 2620, loss: 0.029859986156225204\n",
      "step: 2630, loss: 0.0953778475522995\n",
      "step: 2640, loss: 0.04693754017353058\n",
      "step: 2650, loss: 0.11955235153436661\n",
      "step: 2660, loss: 0.22525867819786072\n",
      "step: 2670, loss: 0.13911570608615875\n",
      "step: 2680, loss: 0.006831922102719545\n",
      "step: 2690, loss: 0.038360193371772766\n",
      "step: 2700, loss: 0.08221384137868881\n",
      "step: 2710, loss: 0.05236225575208664\n",
      "step: 2720, loss: 0.20534133911132812\n",
      "step: 2730, loss: 0.011608856730163097\n",
      "step: 2740, loss: 0.01565735787153244\n",
      "step: 2750, loss: 0.13311465084552765\n",
      "step: 2760, loss: 0.10175193101167679\n",
      "step: 2770, loss: 0.005652443040162325\n",
      "step: 2780, loss: 0.018441669642925262\n",
      "step: 2790, loss: 0.064383864402771\n",
      "step: 2800, loss: 0.055010899901390076\n",
      "step: 2810, loss: 0.16679495573043823\n",
      "step: 2820, loss: 0.05327291786670685\n",
      "step: 2830, loss: 0.03503376990556717\n",
      "step: 2840, loss: 0.040046997368335724\n",
      "step: 2850, loss: 0.17681334912776947\n",
      "step: 2860, loss: 0.11171136796474457\n",
      "step: 2870, loss: 0.03456885740160942\n",
      "step: 2880, loss: 0.022607648745179176\n",
      "step: 2890, loss: 0.013534165918827057\n",
      "step: 2900, loss: 0.03429783880710602\n",
      "step: 2910, loss: 0.16803526878356934\n",
      "step: 2920, loss: 0.032988738268613815\n",
      "step: 2930, loss: 0.13549329340457916\n",
      "step: 2940, loss: 0.028209470212459564\n",
      "step: 2950, loss: 0.012779798358678818\n",
      "step: 2960, loss: 0.02296990342438221\n",
      "step: 2970, loss: 0.07111042737960815\n",
      "step: 2980, loss: 0.07381884753704071\n",
      "step: 2990, loss: 0.10803596675395966\n",
      "step: 3000, loss: 0.017735444009304047\n",
      "step: 3010, loss: 0.012140312232077122\n",
      "step: 3020, loss: 0.006980478763580322\n",
      "step: 3030, loss: 0.11735103279352188\n",
      "step: 3040, loss: 0.0049097612500190735\n",
      "step: 3050, loss: 0.013524532318115234\n",
      "step: 3060, loss: 0.1005498617887497\n",
      "step: 3070, loss: 0.13023389875888824\n",
      "step: 3080, loss: 0.10621900856494904\n",
      "step: 3090, loss: 0.33783847093582153\n",
      "step: 3100, loss: 0.013237969018518925\n",
      "step: 3110, loss: 0.014697140082716942\n",
      "step: 3120, loss: 0.027322635054588318\n",
      "step: 3130, loss: 0.029805580154061317\n",
      "step: 3140, loss: 0.06682752072811127\n",
      "step: 3150, loss: 0.054808974266052246\n",
      "step: 3160, loss: 0.06719702482223511\n",
      "step: 3170, loss: 0.1165941059589386\n",
      "step: 3180, loss: 0.02498045563697815\n",
      "step: 3190, loss: 0.08111602813005447\n",
      "step: 3200, loss: 0.011792458593845367\n",
      "step: 3210, loss: 0.003095628460869193\n",
      "step: 3220, loss: 0.045572057366371155\n",
      "step: 3230, loss: 0.04239869862794876\n",
      "step: 3240, loss: 0.13923627138137817\n",
      "step: 3250, loss: 0.021875187754631042\n",
      "step: 3260, loss: 0.018263621255755424\n",
      "step: 3270, loss: 0.017377380281686783\n",
      "step: 3280, loss: 0.09443638473749161\n",
      "step: 3290, loss: 0.35005083680152893\n",
      "step: 3300, loss: 0.023464733734726906\n",
      "step: 3310, loss: 0.14356449246406555\n",
      "step: 3320, loss: 0.010228408500552177\n",
      "step: 3330, loss: 0.008674310520291328\n",
      "step: 3340, loss: 0.03850710391998291\n",
      "step: 3350, loss: 0.012648757547140121\n",
      "step: 3360, loss: 0.0237564779818058\n",
      "step: 3370, loss: 0.07016332447528839\n",
      "step: 3380, loss: 0.06785836070775986\n",
      "step: 3390, loss: 0.004707236774265766\n",
      "step: 3400, loss: 0.08171556144952774\n",
      "End of dataset\n",
      "Epochs: 1 Elapsed time: 204.9588499069214\n",
      "training done!\n"
     ]
    }
   ],
   "source": [
    "sess = tf.Session(config=sess_config)\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "# train_iterator\n",
    "train_iterator = train_dataset.make_initializable_iterator()\n",
    "train_handle = sess.run(train_iterator.string_handle())\n",
    "\n",
    "# Train\n",
    "max_epochs = 2\n",
    "step = 0\n",
    "for epochs in range(max_epochs):\n",
    "    sess.run(train_iterator.initializer)\n",
    "\n",
    "    start_time = time.time()\n",
    "    while True:\n",
    "        try:\n",
    "            _, loss = sess.run([train_op, cross_entropy],\n",
    "                             feed_dict={handle: train_handle,\n",
    "                                        is_training: True})\n",
    "            if step % 10 == 0:\n",
    "                print(\"step: {}, loss: {}\".format(step, loss))\n",
    "\n",
    "                # summary\n",
    "                summary_str = sess.run(summary_op,\n",
    "                                       feed_dict={handle: train_handle,\n",
    "                                                  is_training: False})\n",
    "                train_writer.add_summary(summary_str, global_step=step)\n",
    "\n",
    "            step += 1\n",
    "\n",
    "        except tf.errors.OutOfRangeError:\n",
    "            print(\"End of dataset\")  # ==> \"End of dataset\"\n",
    "            break\n",
    "\n",
    "    # Save a model per every one epoch in periodically\n",
    "    if epochs % 2 == 0:\n",
    "        print(\"Save model at {} epochs\".format(epochs))\n",
    "        saver.save(sess, train_dir + 'model.ckpt', global_step=step)\n",
    "\n",
    "    print(\"Epochs: {} Elapsed time: {}\".format(epochs, time.time() - start_time))\n",
    "#     print(\"\\n\")\n",
    "\n",
    "train_writer.close()\n",
    "print(\"training done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# restore pretrained tf saver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a saver object for restoring\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoints_dir = 'graphs/train/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "graphs/train/model.ckpt-1704\n",
      "INFO:tensorflow:Restoring parameters from graphs/train/model.ckpt-1704\n"
     ]
    }
   ],
   "source": [
    "sess = tf.Session(config=sess_config)\n",
    "\n",
    "# read the lastest model checkpoint\n",
    "model_ckpt = tf.train.latest_checkpoint(checkpoints_dir)\n",
    "print(model_ckpt)\n",
    "# use saver object to load variables from the saved model\n",
    "saver.restore(sess, model_ckpt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_iterator\n",
    "test_iterator = test_dataset.make_initializable_iterator()\n",
    "test_handle = sess.run(test_iterator.string_handle())\n",
    "sess.run(test_iterator.initializer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test accuracy: 0.9755\n"
     ]
    }
   ],
   "source": [
    "accuracy, acc_op = tf.metrics.accuracy(labels=y, predictions=tf.argmax(logits, 1), name='accuracy')\n",
    "sess.run(tf.local_variables_initializer())\n",
    "\n",
    "sess.run(acc_op, feed_dict={handle: test_handle, is_training: False})\n",
    "print(\"test accuracy:\", sess.run(accuracy, feed_dict={handle: test_handle, is_training: False}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1152x720 with 16 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "test_batch_size = 16\n",
    "batch_index = np.random.choice(len(test_data), size=test_batch_size, replace=False)\n",
    "batch_xs = test_data[batch_index]\n",
    "y_pred = sess.run(logits, feed_dict={x: batch_xs, is_training: False})\n",
    "\n",
    "fig = plt.figure(figsize=(16, 10))\n",
    "for i, (px, py) in enumerate(zip(batch_xs, y_pred)):\n",
    "    p = fig.add_subplot(4, 8, i+1)\n",
    "    p.set_title(\"y_pred: {}\".format(np.argmax(py)))\n",
    "    p.imshow(px.reshape(28, 28), cmap='gray')\n",
    "    p.axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
