{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "test.ipynb",
      "version": "0.3.2",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "TU-rcvm3vmAo",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ZtEmkC05E16P",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        },
        "outputId": "27b1557a-42e3-4138-8bb6-6a07a480f1af"
      },
      "cell_type": "code",
      "source": [
        "from __future__ import division, print_function, absolute_import\n",
        "\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "from tensorflow.examples.tutorials.mnist import input_data\n",
        "mnist = input_data.read_data_sets('MNIST_data/', one_hot=True)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
            "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
            "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
            "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "SFhPuroJwLl3",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "learning_rate_RMSProp = 0.01\n",
        "learning_rate_Gradient_Descent = 0.5\n",
        "training_epochs = 100\n",
        "softmax_classifier_iterations = 1000\n",
        "batch_size = 256\n",
        "display_step = 1\n",
        "examples_to_show = 10\n",
        "n_hidden_1 = 200\n",
        "n_hidden_2 = 200\n",
        "n_input = 784"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "56WYo5yqwT2f",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# learning_rate_RMSProp = 0.01\n",
        "# learning_rate_Gradient_Descent = 0.5\n",
        "# training_epochs = 100\n",
        "# softmax_classifier_iterations = 1000\n",
        "# batch_size = 256\n",
        "# desplay_step = 1\n",
        "# examples_to_show = 10\n",
        "# n_hidden_1 = 200\n",
        "# n_hidden_2 = 200\n",
        "# n_input = 784"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "tdahLU7OwnT6",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# def build_autoencoder():\n",
        "#   Wh_1 = tf.Variable(tf.random_normal([n_input, n_hidden_1]))\n",
        "#   bh_1 = tf.Variable(tf.random_normal([n_hidden_1]))\n",
        "#   h_1 = tf.nn.sigmoid(tf.matmul(X, Wh_1) + bh_1)\n",
        "  \n",
        "#   Wh_2 = tf.Variable(tf.random_normal([n_hidden_1, n_hidden_2]))\n",
        "#   bh_2 = tf.Variable(tf.random_normal([n_hidden_2]))\n",
        "#   h_2 = tf.nn.sigmoid(tf.matmul(h_1, Wh_2) + bh_2)\n",
        "  \n",
        "#   Wo = tf.Variable(tf.random_normal([n_hidden_2, n_input]))\n",
        "#   bo = tf.Variable(tf.random_normal([n_input]))\n",
        "#   X_reconstructed = tf.nn.sigmoid(tf.matmul(h_2, Wo) + bo)\n",
        "  \n",
        "#   return X_reconstructed, h_2"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "opHx431tFoqj",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def build_autoencoder():\n",
        "  Wh_1 = tf.Variable(tf.random_normal([n_input, n_hidden_1]))\n",
        "  bh_1 = tf.Variable(tf.random_normal([n_hidden_1]))\n",
        "  h_1 = tf.nn.sigmoid(tf.matmul(X, Wh_1) + bh_1)\n",
        "  \n",
        "  Wh_2 = tf.Variable(tf.random_normal([n_hidden_1, n_hidden_2]))\n",
        "  bh_2 = tf.Variable(tf.random_normal([n_hidden_2]))\n",
        "  h_2 = tf.nn.sigmoid(tf.matmul(h_1, Wh_2) + bh_2)\n",
        "  \n",
        "  Wo = tf.Variable(tf.random_normal([n_hidden_2, n_input]))\n",
        "  bo = tf.Variable(tf.random_normal([n_input]))\n",
        "  X_reconstructed = tf.nn.sigmoid(tf.matmul(h_2, Wo) + bo)\n",
        "  \n",
        "  return X_reconstructed, h_2"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_iJET407x5VK",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# def build_softmax_classifier():\n",
        "#   W = tf.Variable(tf.zeros([n_hidden_2, 10]))\n",
        "#   b = tf.Variable(tf.zeros([10]))\n",
        "#   y_pred = tf.nn.softmax(tf.matmul(extracted_features, W) + b)\n",
        "  \n",
        "#   return y_pred"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "l1Y9i1gdGraH",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def build_softmax_classifier():\n",
        "  W = tf.Variable(tf.zeros([n_hidden_2, 10]))\n",
        "  b = tf.Variable(tf.zeros([10]))\n",
        "  y_pred = tf.nn.softmax(tf.matmul(extracted_features, W) + b)\n",
        "  \n",
        "  return y_pred"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "5LyMCAnSyJbl",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# X = tf.placeholder('float', [None, n_input])\n",
        "# y_pred, extracted_features = build_autoencoder()\n",
        "\n",
        "# y_true = X\n",
        "# y = build_softmax_classifier()\n",
        "# y_ = tf.placeholder(tf.float32, [None, 10])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Y9KsZ8x-HEkp",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "X = tf.placeholder('float', [None, n_input])\n",
        "###y_pred = softmax result, extracted_features = hidden layer2 output\n",
        "y_pred, extracted_features = build_autoencoder()\n",
        "\n",
        "y_true = X\n",
        "y = build_softmax_classifier()\n",
        "y_ = tf.placeholder(tf.float32, [None, 10])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "atmh7ke8yajV",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# reconstruction_cost = tf.reduce_mean(tf.pow(y_true - y_pred, 2))\n",
        "# initial_optimizer = tf.train.RMSPropOptimizer(learning_rate_RMSProp).minimize(reconstruction_cost)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "xvDSaRwAy1qf",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# cross_entropy_cost = tf.reduce_mean(-tf.reduce_sum(y_ * tf.log(y), reduction_indices=[1]))\n",
        "# softmax_classifier_optimizer = tf.train.GradientDescentOptimizer(learning_rate_Gradient_Descent).minimize(cross_entropy_cost)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "x2jBAtZizZRc",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# finetuning_cost = cross_entropy_cost + reconstruction_cost\n",
        "# finetuning_optimizer = tf.train.GradientDescentOptimizer(learning_rate_Gradient_Descent).minimize(finetuning_cost)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "nyXKi9poH5pJ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "reconstruction_cost = tf.reduce_mean(tf.pow(y_true - y_pred, 2))\n",
        "initial_optimizer = tf.train.RMSPropOptimizer(learning_rate_RMSProp).minimize(reconstruction_cost)\n",
        "\n",
        "cross_entropy_cost = tf.reduce_mean(-tf.reduce_sum(y_ * tf.log(y), reduction_indices=[1]))\n",
        "softmax_classifier_optimizer = tf.train.GradientDescentOptimizer(learning_rate_Gradient_Descent).minimize(cross_entropy_cost)\n",
        "\n",
        "finetuning_cost = cross_entropy_cost + reconstruction_cost\n",
        "finetuning_optimizer = tf.train.GradientDescentOptimizer(learning_rate_Gradient_Descent).minimize(finetuning_cost)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ilMutjLtz30g",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# print('Training Start...')\n",
        "# import time\n",
        "\n",
        "# start = time.time()\n",
        "# total = time.time()\n",
        "\n",
        "# with tf.Session() as sess:\n",
        "#   sess.run(tf.global_variables_initializer())\n",
        "  \n",
        "#   total_batch = int(mnist.train.num_examples / batch_size)\n",
        "  \n",
        "#   for epoch in range(training_epochs):\n",
        "#     for i in range(total_batch):\n",
        "#       batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
        "#       _, cost_value = sess.run([initial_optimizer, reconstruction_cost], \n",
        "#                                feed_dict={X: batch_xs})\n",
        "      \n",
        "#     if epoch % 100 == 0:\n",
        "#       print('Epoch step: {}, Loss: {}, Duration: {:.3f}'.format(epoch+1, cost_value, time.time() - start))\n",
        "#       start = time.time()\n",
        "#   print('pre-train optimization finished...')\n",
        "  \n",
        "#   reconstructed_image = sess.run(y_pred, feed_dict={X: mnist.test.images[:examples_to_show]})\n",
        "#   f, a = plt.subplots(2, 10, figsize=(10, 2))\n",
        "#   for i in range(examples_to_show):\n",
        "#       a[0][i].imshow(np.reshape(mnist.test.images[i], (28, 28)))\n",
        "#       a[1][i].imshow(np.reshape(reconstructed_image[i], (28, 28)))\n",
        "    \n",
        "#   f.show()\n",
        "#   plt.draw()\n",
        "#   f.savefig('image.png')\n",
        "  \n",
        "#   for i in range(softmax_classifier_iterations):\n",
        "#     batch_xs, batch_ys = mnist.train.next_batch(100)  \n",
        "#     sess.run(softmax_classifier_optimizer, feed_dict={X: batch_xs, y_: batch_ys})\n",
        "#   print('classifier optimization finisehd...')\n",
        "  \n",
        "#   correct_prediction = tf.equal(tf.argmax(y,1), tf.argmax(y_, 1))\n",
        "#   accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
        "#   print('accuray(before fine-tuning): ')\n",
        "#   print(sess.run(accuracy, feed_dict={X: mnist.test.images, y_: mnist.test.labels}))\n",
        "  \n",
        "#   start = time.time()\n",
        "#   for epoch in range(training_epochs):\n",
        "#     for i in range(total_batch):\n",
        "#       batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
        "#       _, cost_value = sess.run([finetuning_optimizer, finetuning_cost], feed_dict={X: batch_xs, y_: batch_ys})\n",
        "      \n",
        "# #       if epoch % 100 == 0:\n",
        "# #         print('Epoch : {}, Loss: {}, Duration : {:.3f}'.format(epoch, cost_value, time.time() - start))\n",
        "# #         start = time.time()\n",
        "#   print('fine-tunintg finished...')\n",
        "  \n",
        "#   print('accuracy(after fine-tuning):')\n",
        "#   print(sess.run(accuracy, feed_dict={X: mnist.test.images, y_: mnist.test.labels}))\n",
        "#   print('Elapsed time: {:.3f}'.format(time.time() - total))\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "CtagSQuBJGt1",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "dZ21tivJ6q_L",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 215
        },
        "outputId": "c582b8ae-e967-43af-d7d3-5bebc7b02ac8"
      },
      "cell_type": "code",
      "source": [
        "print('Training start...')\n",
        "import time\n",
        "\n",
        "start = time.time()\n",
        "total = time.time()\n",
        "\n",
        "with tf.Session() as sess:\n",
        "  sess.run(tf.global_variables_initializer())\n",
        "  \n",
        "  total_batch = int(mnist.train.num_examples / batch_size)\n",
        "  \n",
        "  for epoch in range(training_epochs):\n",
        "    for i in range(total_batch):\n",
        "      batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
        "      _, cost_value = sess.run([initial_optimizer, reconstruction_cost], feed_dict = {X: batch_xs})\n",
        "      \n",
        "    if epoch % 50 == 0:\n",
        "      print('Epoch step: {}, Loss: {:.5f}, Duration: {:.3f}'.format(epoch, cost_value, time.time() - start))\n",
        "      \n",
        "  print('pre-train optimization finished...')\n",
        "  \n",
        "#   reconstructed_image = sess.run(y_pred, feed_dict={X: mnist.test.images[:examples_to_show]})\n",
        "#   f, a = plt.subplots(2, 10, figsize=(10, 2))\n",
        "#   for i in range(examples_to_show):\n",
        "#     a[0][i].imshow(np.reshape(mnist.test.images[i], (28, 28)))\n",
        "#     a[1][i].imshow(np.reshape(reconstructed_image[i], (28, 28)))\n",
        "    \n",
        "#   f.show()\n",
        "#   plt.draw()\n",
        "#   f.savefig('image.png')\n",
        "  \n",
        "  for i in range(softmax_classifier_iterations):\n",
        "    batch_xs, batch_ys = mnist.train.next_batch(100)\n",
        "    sess.run(softmax_classifier_optimizer, feed_dict={X: batch_xs, y_: batch_ys})\n",
        "    \n",
        "  print('classifier optimization finishied...')\n",
        "  \n",
        "  correct_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(y_, 1))\n",
        "  accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
        "  accu = sess.run(accuracy, feed_dict = {X: mnist.test.images, y_: mnist.test.labels})\n",
        "  print('accuracy(before fine-tuning): {}'.format(accu))\n",
        "  \n",
        "  start = time.time()\n",
        "  for epoch in range(training_epochs):\n",
        "    for i in range(total_batch):\n",
        "      batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
        "      _, cost_value = sess.run([finetuning_optimizer, finetuning_cost], feed_dict={X: batch_xs, y_: batch_ys})\n",
        "  \n",
        "    if epoch % 50 == 0:\n",
        "      print('Epoch: {}, Loss: {}, Duration: {:.3f}'.format(epoch, cost_value, time.time() - start))\n",
        "      start = time.time()\n",
        "  \n",
        "  print('fine-tuning finished...')\n",
        "  \n",
        "  accu = sess.run(accuracy, feed_dict={X: mnist.test.images, y_: mnist.test.labels})\n",
        "  print('accuracy(after fine-tuning): {}'.format(accu))\n",
        "  print('Elapsed time: {:.3f}'.format(time.time() - total))"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training start...\n",
            "Epoch step: 0, Loss: 0.15401, Duration: 0.762\n",
            "Epoch step: 50, Loss: 0.06633, Duration: 33.016\n",
            "pre-train optimization finished...\n",
            "classifier optimization finishied...\n",
            "accuracy(before fine-tuning): 0.9132999777793884\n",
            "Epoch: 0, Loss: 0.30908283591270447, Duration: 0.834\n",
            "Epoch: 50, Loss: 0.13354361057281494, Duration: 38.529\n",
            "fine-tuning finished...\n",
            "accuracy(after fine-tuning): 0.9610999822616577\n",
            "Elapsed time: 144.417\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "vqjqsIAfOEQY",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}